{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DKbA4RgbrMbu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Admission_Predict_Ver1.1.csv')"
      ],
      "metadata": {
        "id": "ya9ItuVRrrGQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRL-G1qxrxIv",
        "outputId": "1b43a664-63c8-4a88-f038-85e8d38c7eec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qQgAgl2Ir3Up",
        "outputId": "eabb7a03-8e92-4e91-c7a1-3c3df9578696"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
              "0           1        337          118                  4  4.5   4.5  9.65   \n",
              "1           2        324          107                  4  4.0   4.5  8.87   \n",
              "2           3        316          104                  3  3.0   3.5  8.00   \n",
              "3           4        322          110                  3  3.5   2.5  8.67   \n",
              "4           5        314          103                  2  2.0   3.0  8.21   \n",
              "\n",
              "   Research  Chance of Admit   \n",
              "0         1              0.92  \n",
              "1         1              0.76  \n",
              "2         1              0.72  \n",
              "3         1              0.80  \n",
              "4         0              0.65  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a5a286c-1126-4fb1-9eef-00cf5e878ee8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>337</td>\n",
              "      <td>118</td>\n",
              "      <td>4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>9.65</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>324</td>\n",
              "      <td>107</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>316</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>322</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>8.67</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>314</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a5a286c-1126-4fb1-9eef-00cf5e878ee8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a5a286c-1126-4fb1-9eef-00cf5e878ee8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a5a286c-1126-4fb1-9eef-00cf5e878ee8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"Serial No.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 144,\n        \"min\": 1,\n        \"max\": 500,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          362,\n          74,\n          375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GRE Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 290,\n        \"max\": 340,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          307,\n          335,\n          297\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TOEFL Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 92,\n        \"max\": 120,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          94,\n          119,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"University Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9910036207566069,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.0,\n          4.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LOR \",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9254495738978181,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          5.0,\n          3.5,\n          1.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CGPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6048128003332052,\n        \"min\": 6.8,\n        \"max\": 9.92,\n        \"num_unique_values\": 184,\n        \"samples\": [\n          9.6,\n          8.9,\n          8.24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chance of Admit \",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1411404039503023,\n        \"min\": 0.34,\n        \"max\": 0.97,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          0.92,\n          0.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "gZl0l36Jr7h5",
        "outputId": "f51f4191-9b7f-436e-e64d-f4f9be44e0c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Serial No.   GRE Score  TOEFL Score  University Rating         SOP  \\\n",
              "count  500.000000  500.000000   500.000000         500.000000  500.000000   \n",
              "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
              "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
              "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
              "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
              "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
              "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
              "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
              "\n",
              "            LOR         CGPA    Research  Chance of Admit   \n",
              "count  500.00000  500.000000  500.000000         500.00000  \n",
              "mean     3.48400    8.576440    0.560000           0.72174  \n",
              "std      0.92545    0.604813    0.496884           0.14114  \n",
              "min      1.00000    6.800000    0.000000           0.34000  \n",
              "25%      3.00000    8.127500    0.000000           0.63000  \n",
              "50%      3.50000    8.560000    1.000000           0.72000  \n",
              "75%      4.00000    9.040000    1.000000           0.82000  \n",
              "max      5.00000    9.920000    1.000000           0.97000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4224c46-c745-412b-baaa-cc973c0cc02d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.00000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>250.500000</td>\n",
              "      <td>316.472000</td>\n",
              "      <td>107.192000</td>\n",
              "      <td>3.114000</td>\n",
              "      <td>3.374000</td>\n",
              "      <td>3.48400</td>\n",
              "      <td>8.576440</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.72174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>144.481833</td>\n",
              "      <td>11.295148</td>\n",
              "      <td>6.081868</td>\n",
              "      <td>1.143512</td>\n",
              "      <td>0.991004</td>\n",
              "      <td>0.92545</td>\n",
              "      <td>0.604813</td>\n",
              "      <td>0.496884</td>\n",
              "      <td>0.14114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>290.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>6.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.34000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>125.750000</td>\n",
              "      <td>308.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>8.127500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.63000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>250.500000</td>\n",
              "      <td>317.000000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.50000</td>\n",
              "      <td>8.560000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.72000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>375.250000</td>\n",
              "      <td>325.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>9.040000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.82000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>340.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.00000</td>\n",
              "      <td>9.920000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.97000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4224c46-c745-412b-baaa-cc973c0cc02d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4224c46-c745-412b-baaa-cc973c0cc02d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4224c46-c745-412b-baaa-cc973c0cc02d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Serial No.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 179.8977277873755,\n        \"min\": 1.0,\n        \"max\": 500.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          500.0,\n          250.5,\n          375.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GRE Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 134.31959598717793,\n        \"min\": 11.295148372354694,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          316.472,\n          317.0,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TOEFL Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148.54698537663884,\n        \"min\": 6.081867659564522,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          107.192,\n          107.0,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"University Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 175.8093363236959,\n        \"min\": 1.0,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.114,\n          3.0,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 175.75364204315028,\n        \"min\": 0.9910036207566069,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.374,\n          3.5,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LOR \",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 175.72621272918164,\n        \"min\": 0.9254495738978181,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.484,\n          3.5,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CGPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 174.19317432229437,\n        \"min\": 0.6048128003332052,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          8.576439999999998,\n          8.56,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176.57228090801308,\n        \"min\": 0.0,\n        \"max\": 500.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.56,\n          1.0,\n          0.49688407860903494\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chance of Admit \",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176.55754521339983,\n        \"min\": 0.1411404039503023,\n        \"max\": 500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.72174,\n          0.72,\n          500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q48gctPNsEZf",
        "outputId": "da0e41f8-5037-4c42-e260-703c25b2bcfe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   Serial No.         500 non-null    int64  \n",
            " 1   GRE Score          500 non-null    int64  \n",
            " 2   TOEFL Score        500 non-null    int64  \n",
            " 3   University Rating  500 non-null    int64  \n",
            " 4   SOP                500 non-null    float64\n",
            " 5   LOR                500 non-null    float64\n",
            " 6   CGPA               500 non-null    float64\n",
            " 7   Research           500 non-null    int64  \n",
            " 8   Chance of Admit    500 non-null    float64\n",
            "dtypes: float64(4), int64(5)\n",
            "memory usage: 35.3 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "iw4Jr4iZsFYA",
        "outputId": "7c080f28-c752-472c-c44d-2e1992374d0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Serial No.           0\n",
              "GRE Score            0\n",
              "TOEFL Score          0\n",
              "University Rating    0\n",
              "SOP                  0\n",
              "LOR                  0\n",
              "CGPA                 0\n",
              "Research             0\n",
              "Chance of Admit      0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Serial No.</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GRE Score</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOEFL Score</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>University Rating</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SOP</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOR</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CGPA</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Research</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Chance of Admit</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Serial No.', axis=1)"
      ],
      "metadata": {
        "id": "eemGUi9BsG-D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XKlqe7wGsMxg",
        "outputId": "7dd00645-a544-48db-ffed-19cca7532350"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
              "0        337          118                  4  4.5   4.5  9.65         1   \n",
              "1        324          107                  4  4.0   4.5  8.87         1   \n",
              "2        316          104                  3  3.0   3.5  8.00         1   \n",
              "3        322          110                  3  3.5   2.5  8.67         1   \n",
              "4        314          103                  2  2.0   3.0  8.21         0   \n",
              "\n",
              "   Chance of Admit   \n",
              "0              0.92  \n",
              "1              0.76  \n",
              "2              0.72  \n",
              "3              0.80  \n",
              "4              0.65  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a44d3cf5-5fb2-4729-b4ea-2690d72072a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>337</td>\n",
              "      <td>118</td>\n",
              "      <td>4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>9.65</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>324</td>\n",
              "      <td>107</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>316</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>322</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>8.67</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>314</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a44d3cf5-5fb2-4729-b4ea-2690d72072a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a44d3cf5-5fb2-4729-b4ea-2690d72072a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a44d3cf5-5fb2-4729-b4ea-2690d72072a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"GRE Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 290,\n        \"max\": 340,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          307,\n          335,\n          297\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TOEFL Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 92,\n        \"max\": 120,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          94,\n          119,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"University Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9910036207566069,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          1.0,\n          4.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LOR \",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9254495738978181,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          5.0,\n          3.5,\n          1.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CGPA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6048128003332052,\n        \"min\": 6.8,\n        \"max\": 9.92,\n        \"num_unique_values\": 184,\n        \"samples\": [\n          9.6,\n          8.9,\n          8.24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Research\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Chance of Admit \",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1411404039503023,\n        \"min\": 0.34,\n        \"max\": 0.97,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          0.92,\n          0.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Chance of Admit ', axis=1)\n",
        "y = df['Chance of Admit ']"
      ],
      "metadata": {
        "id": "K-JjerN7sNqv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIH0uwYRsaa4",
        "outputId": "c44442c0-b281-4cc4-e000-db84e6fc932f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GnwvrWvscIo",
        "outputId": "c1e80c4f-e3b8-426a-c16e-c6080dfcc929"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9JQj_Sgnsc3P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
      ],
      "metadata": {
        "id": "tXJgLMQ_shAq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WRLmCtqtKFk",
        "outputId": "748f5c04-2f45-44d9-a717-cc97dc35fd93"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhx8F1MotMFi",
        "outputId": "c5162b1a-d4de-4fb6-8de5-c85cd509116f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scalling"
      ],
      "metadata": {
        "id": "zn3oeAd9tPu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "rMoqjfexsqyK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "YPtpfmHhs0PD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNLSQdg7s90J",
        "outputId": "147ddce4-4e8d-467b-8eb0-0c1c37edb7a1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.54      , 0.39285714, 0.25      , 0.375     , 0.25      ,\n",
              "        0.43269231, 0.        ],\n",
              "       [0.44      , 0.60714286, 0.25      , 0.375     , 0.75      ,\n",
              "        0.71153846, 0.        ],\n",
              "       [0.78      , 0.78571429, 0.25      , 0.25      , 0.75      ,\n",
              "        0.56410256, 1.        ],\n",
              "       [0.96      , 1.        , 0.75      , 1.        , 1.        ,\n",
              "        0.91666667, 1.        ],\n",
              "       [0.66      , 0.53571429, 0.75      , 0.5       , 0.375     ,\n",
              "        0.53846154, 1.        ],\n",
              "       [0.42      , 0.35714286, 0.5       , 0.875     , 0.75      ,\n",
              "        0.58974359, 1.        ],\n",
              "       [0.36      , 0.46428571, 0.75      , 0.5       , 0.375     ,\n",
              "        0.36858974, 1.        ],\n",
              "       [0.36      , 0.57142857, 0.75      , 0.875     , 1.        ,\n",
              "        0.49358974, 0.        ],\n",
              "       [0.36      , 0.64285714, 0.5       , 0.625     , 0.5       ,\n",
              "        0.38461538, 1.        ],\n",
              "       [0.68      , 0.75      , 0.75      , 0.875     , 0.75      ,\n",
              "        0.63782051, 0.        ],\n",
              "       [0.52      , 0.35714286, 0.5       , 0.25      , 0.5       ,\n",
              "        0.19230769, 0.        ],\n",
              "       [0.1       , 0.14285714, 0.25      , 0.125     , 0.25      ,\n",
              "        0.17307692, 0.        ],\n",
              "       [0.74      , 0.85714286, 0.75      , 0.75      , 0.875     ,\n",
              "        0.85897436, 1.        ],\n",
              "       [0.68      , 0.64285714, 0.5       , 0.625     , 0.75      ,\n",
              "        0.66346154, 1.        ],\n",
              "       [0.52      , 0.64285714, 0.5       , 0.625     , 0.75      ,\n",
              "        0.56410256, 0.        ],\n",
              "       [0.38      , 0.5       , 0.25      , 0.375     , 0.375     ,\n",
              "        0.38461538, 0.        ],\n",
              "       [0.5       , 0.46428571, 0.5       , 0.25      , 0.375     ,\n",
              "        0.49358974, 0.        ],\n",
              "       [0.84      , 0.85714286, 1.        , 1.        , 1.        ,\n",
              "        0.79487179, 1.        ],\n",
              "       [0.68      , 0.82142857, 0.5       , 0.625     , 0.5       ,\n",
              "        0.62820513, 1.        ],\n",
              "       [0.7       , 0.71428571, 0.75      , 0.75      , 0.75      ,\n",
              "        0.70512821, 1.        ],\n",
              "       [0.66      , 0.64285714, 0.5       , 0.75      , 0.625     ,\n",
              "        0.73717949, 1.        ],\n",
              "       [0.38      , 0.42857143, 0.25      , 0.25      , 0.375     ,\n",
              "        0.46794872, 0.        ],\n",
              "       [0.6       , 1.        , 0.5       , 0.75      , 0.875     ,\n",
              "        0.74038462, 0.        ],\n",
              "       [0.54      , 0.64285714, 0.5       , 0.75      , 0.875     ,\n",
              "        0.74038462, 1.        ],\n",
              "       [0.92      , 0.71428571, 1.        , 1.        , 1.        ,\n",
              "        0.94871795, 1.        ],\n",
              "       [0.4       , 0.42857143, 0.5       , 0.25      , 0.625     ,\n",
              "        0.50320513, 0.        ],\n",
              "       [0.5       , 0.42857143, 0.5       , 0.75      , 0.375     ,\n",
              "        0.41666667, 0.        ],\n",
              "       [0.36      , 0.60714286, 0.25      , 0.5       , 0.75      ,\n",
              "        0.52884615, 0.        ],\n",
              "       [0.56      , 0.5       , 0.25      , 0.75      , 0.75      ,\n",
              "        0.35897436, 1.        ],\n",
              "       [0.14      , 0.21428571, 0.25      , 0.375     , 0.5       ,\n",
              "        0.27884615, 0.        ],\n",
              "       [0.46      , 0.32142857, 0.5       , 0.375     , 0.5       ,\n",
              "        0.3974359 , 0.        ],\n",
              "       [0.44      , 0.42857143, 0.5       , 0.625     , 0.625     ,\n",
              "        0.51923077, 0.        ],\n",
              "       [0.8       , 0.78571429, 0.5       , 0.875     , 0.875     ,\n",
              "        0.78205128, 1.        ],\n",
              "       [0.58      , 0.71428571, 0.5       , 0.375     , 0.25      ,\n",
              "        0.61217949, 1.        ],\n",
              "       [0.7       , 0.64285714, 0.75      , 0.875     , 0.75      ,\n",
              "        0.69230769, 1.        ],\n",
              "       [0.66      , 0.64285714, 1.        , 0.75      , 1.        ,\n",
              "        0.69871795, 1.        ],\n",
              "       [0.24      , 0.25      , 0.25      , 0.        , 0.25      ,\n",
              "        0.375     , 0.        ],\n",
              "       [0.4       , 0.53571429, 0.5       , 0.625     , 0.625     ,\n",
              "        0.59935897, 0.        ],\n",
              "       [0.48      , 0.35714286, 0.75      , 0.375     , 0.25      ,\n",
              "        0.34615385, 1.        ],\n",
              "       [0.68      , 0.75      , 0.75      , 0.875     , 0.875     ,\n",
              "        0.78525641, 1.        ],\n",
              "       [0.58      , 0.39285714, 0.5       , 0.375     , 0.75      ,\n",
              "        0.62820513, 1.        ],\n",
              "       [0.44      , 0.32142857, 0.25      , 0.375     , 0.625     ,\n",
              "        0.3974359 , 1.        ],\n",
              "       [0.62      , 0.67857143, 0.75      , 0.75      , 0.75      ,\n",
              "        0.69551282, 1.        ],\n",
              "       [0.76      , 0.85714286, 1.        , 0.875     , 1.        ,\n",
              "        0.73076923, 1.        ],\n",
              "       [0.42      , 0.5       , 0.25      , 0.625     , 0.5       ,\n",
              "        0.46794872, 1.        ],\n",
              "       [0.82      , 0.71428571, 1.        , 0.75      , 1.        ,\n",
              "        0.96153846, 1.        ],\n",
              "       [0.94      , 0.89285714, 1.        , 1.        , 1.        ,\n",
              "        0.98397436, 1.        ],\n",
              "       [0.44      , 0.46428571, 0.25      , 0.125     , 0.5       ,\n",
              "        0.53205128, 0.        ],\n",
              "       [0.46      , 0.21428571, 0.5       , 0.375     , 0.875     ,\n",
              "        0.48076923, 1.        ],\n",
              "       [0.12      , 0.32142857, 0.        , 0.375     , 0.5       ,\n",
              "        0.28205128, 0.        ],\n",
              "       [0.44      , 0.53571429, 0.75      , 0.875     , 0.75      ,\n",
              "        0.59294872, 1.        ],\n",
              "       [0.62      , 0.60714286, 0.5       , 0.625     , 0.625     ,\n",
              "        0.64102564, 1.        ],\n",
              "       [0.42      , 0.42857143, 0.5       , 0.875     , 0.875     ,\n",
              "        0.5224359 , 0.        ],\n",
              "       [0.1       , 0.03571429, 0.        , 0.25      , 0.25      ,\n",
              "        0.12820513, 0.        ],\n",
              "       [0.14      , 0.14285714, 0.25      , 0.375     , 0.125     ,\n",
              "        0.34935897, 0.        ],\n",
              "       [0.84      , 0.96428571, 0.75      , 1.        , 0.875     ,\n",
              "        0.78205128, 1.        ],\n",
              "       [0.84      , 0.89285714, 0.75      , 0.875     , 0.75      ,\n",
              "        0.73717949, 0.        ],\n",
              "       [0.66      , 0.75      , 0.5       , 0.75      , 0.75      ,\n",
              "        0.66666667, 1.        ],\n",
              "       [0.3       , 0.42857143, 0.25      , 0.375     , 0.125     ,\n",
              "        0.31730769, 0.        ],\n",
              "       [0.26      , 0.46428571, 1.        , 1.        , 0.875     ,\n",
              "        0.59294872, 0.        ],\n",
              "       [0.52      , 0.39285714, 0.25      , 0.25      , 0.875     ,\n",
              "        0.62179487, 0.        ],\n",
              "       [0.22      , 0.25      , 0.5       , 0.375     , 0.25      ,\n",
              "        0.52884615, 1.        ],\n",
              "       [0.3       , 0.14285714, 0.75      , 0.5       , 0.875     ,\n",
              "        0.46794872, 0.        ],\n",
              "       [0.24      , 0.32142857, 0.25      , 0.375     , 0.625     ,\n",
              "        0.37179487, 0.        ],\n",
              "       [0.34      , 0.46428571, 0.75      , 0.5       , 0.5       ,\n",
              "        0.36538462, 0.        ],\n",
              "       [0.36      , 0.57142857, 0.5       , 0.625     , 0.625     ,\n",
              "        0.45512821, 0.        ],\n",
              "       [0.2       , 0.21428571, 0.        , 0.25      , 0.375     ,\n",
              "        0.39102564, 0.        ],\n",
              "       [0.68      , 0.60714286, 0.5       , 0.625     , 0.5       ,\n",
              "        0.68589744, 1.        ],\n",
              "       [0.44      , 0.42857143, 0.5       , 0.625     , 0.75      ,\n",
              "        0.41346154, 0.        ],\n",
              "       [0.56      , 0.60714286, 0.5       , 0.5       , 0.5       ,\n",
              "        0.54487179, 0.        ],\n",
              "       [0.68      , 0.71428571, 0.75      , 0.875     , 0.75      ,\n",
              "        0.77564103, 1.        ],\n",
              "       [0.24      , 0.25      , 0.5       , 0.375     , 0.5       ,\n",
              "        0.20833333, 0.        ],\n",
              "       [0.22      , 0.35714286, 0.5       , 0.375     , 0.25      ,\n",
              "        0.42628205, 1.        ],\n",
              "       [0.44      , 0.60714286, 0.5       , 0.5       , 0.5       ,\n",
              "        0.60576923, 0.        ],\n",
              "       [0.18      , 0.35714286, 0.5       , 0.75      , 0.625     ,\n",
              "        0.58333333, 0.        ],\n",
              "       [0.42      , 0.25      , 0.        , 0.375     , 0.5       ,\n",
              "        0.5224359 , 1.        ],\n",
              "       [0.74      , 0.78571429, 0.5       , 0.5       , 0.5       ,\n",
              "        0.71153846, 0.        ],\n",
              "       [0.74      , 0.75      , 0.5       , 0.625     , 0.5       ,\n",
              "        0.59615385, 1.        ],\n",
              "       [0.72      , 0.71428571, 0.75      , 0.75      , 0.625     ,\n",
              "        0.74358974, 1.        ],\n",
              "       [0.22      , 0.42857143, 0.25      , 0.625     , 0.625     ,\n",
              "        0.34935897, 1.        ],\n",
              "       [0.68      , 0.46428571, 0.5       , 0.5       , 0.75      ,\n",
              "        0.625     , 0.        ],\n",
              "       [0.7       , 0.78571429, 0.75      , 0.5       , 0.25      ,\n",
              "        0.51282051, 0.        ],\n",
              "       [0.52      , 0.35714286, 0.25      , 0.75      , 0.625     ,\n",
              "        0.43269231, 0.        ],\n",
              "       [0.78      , 0.96428571, 0.75      , 0.875     , 0.875     ,\n",
              "        0.75641026, 1.        ],\n",
              "       [0.96      , 0.89285714, 0.75      , 0.625     , 0.875     ,\n",
              "        0.8525641 , 1.        ],\n",
              "       [0.88      , 0.85714286, 0.75      , 0.75      , 0.5       ,\n",
              "        0.38461538, 1.        ],\n",
              "       [0.4       , 0.5       , 0.75      , 0.125     , 0.375     ,\n",
              "        0.5       , 0.        ],\n",
              "       [0.52      , 0.21428571, 0.        , 0.125     , 0.25      ,\n",
              "        0.20192308, 0.        ],\n",
              "       [0.5       , 0.64285714, 0.25      , 0.625     , 0.5       ,\n",
              "        0.53205128, 1.        ],\n",
              "       [0.88      , 0.78571429, 0.75      , 0.75      , 0.75      ,\n",
              "        0.84294872, 1.        ],\n",
              "       [0.4       , 0.5       , 0.25      , 0.625     , 0.375     ,\n",
              "        0.49038462, 0.        ],\n",
              "       [1.        , 0.78571429, 1.        , 0.75      , 0.75      ,\n",
              "        0.8974359 , 1.        ],\n",
              "       [0.2       , 0.32142857, 0.5       , 0.625     , 0.375     ,\n",
              "        0.34615385, 0.        ],\n",
              "       [0.3       , 0.71428571, 0.5       , 0.5       , 0.625     ,\n",
              "        0.59294872, 0.        ],\n",
              "       [0.58      , 0.57142857, 0.5       , 0.5       , 0.625     ,\n",
              "        0.55769231, 1.        ],\n",
              "       [0.6       , 0.75      , 0.25      , 0.25      , 0.375     ,\n",
              "        0.58974359, 1.        ],\n",
              "       [0.8       , 0.75      , 1.        , 1.        , 0.75      ,\n",
              "        0.80448718, 1.        ],\n",
              "       [0.68      , 0.64285714, 0.75      , 0.875     , 0.75      ,\n",
              "        0.75320513, 1.        ],\n",
              "       [0.84      , 0.92857143, 0.25      , 0.875     , 0.625     ,\n",
              "        0.82051282, 1.        ],\n",
              "       [0.78      , 0.64285714, 0.25      , 0.75      , 0.5       ,\n",
              "        0.75320513, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzUvUVpetCV6",
        "outputId": "f9d856a3-99f7-4889-9e06-1ef3c1f79439"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.42      , 0.42857143, 0.25      , ..., 0.625     , 0.53846154,\n",
              "        0.        ],\n",
              "       [0.48      , 0.35714286, 0.25      , ..., 0.375     , 0.46153846,\n",
              "        0.        ],\n",
              "       [0.54      , 0.53571429, 0.5       , ..., 0.5       , 0.6025641 ,\n",
              "        1.        ],\n",
              "       ...,\n",
              "       [0.84      , 0.57142857, 1.        , ..., 0.75      , 0.71153846,\n",
              "        1.        ],\n",
              "       [0.5       , 0.25      , 0.25      , ..., 0.5       , 0.34935897,\n",
              "        0.        ],\n",
              "       [0.54      , 0.5       , 0.25      , ..., 0.625     , 0.42307692,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "ixN5i3MOtUNT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(7, activation='relu', input_dim=7))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD_A61YDthnC",
        "outputId": "bcb14c75-73d8-4c40-cbea-da80ed9ecaea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "bz8Atqmpt1mD",
        "outputId": "63561a16-bca5-4194-dc46-db4d33f3786c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                          \u001b[38;5;34m56\u001b[0m \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                          \u001b[38;5;34m56\u001b[0m \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                           \u001b[38;5;34m8\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "GPb6TDYOt3kV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=500, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkC6dcdcuBfj",
        "outputId": "d69d8e9d-8305-4925-8da0-f142f7691422"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.4258 - val_loss: 0.3469\n",
            "Epoch 2/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3392 - val_loss: 0.2599\n",
            "Epoch 3/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2562 - val_loss: 0.1920\n",
            "Epoch 4/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1811 - val_loss: 0.1402\n",
            "Epoch 5/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1330 - val_loss: 0.1009\n",
            "Epoch 6/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0976 - val_loss: 0.0717\n",
            "Epoch 7/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0668 - val_loss: 0.0506\n",
            "Epoch 8/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0481 - val_loss: 0.0359\n",
            "Epoch 9/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0317 - val_loss: 0.0261\n",
            "Epoch 10/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0235 - val_loss: 0.0200\n",
            "Epoch 11/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0176 - val_loss: 0.0164\n",
            "Epoch 12/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0158 - val_loss: 0.0144\n",
            "Epoch 13/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0134 - val_loss: 0.0134\n",
            "Epoch 14/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0126 - val_loss: 0.0128\n",
            "Epoch 15/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0120 - val_loss: 0.0125\n",
            "Epoch 16/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0141 - val_loss: 0.0123\n",
            "Epoch 17/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0111 - val_loss: 0.0121\n",
            "Epoch 18/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0121 - val_loss: 0.0120\n",
            "Epoch 19/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0115 - val_loss: 0.0118\n",
            "Epoch 20/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0129 - val_loss: 0.0116\n",
            "Epoch 21/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0119 - val_loss: 0.0115\n",
            "Epoch 22/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0106 - val_loss: 0.0113\n",
            "Epoch 23/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0105 - val_loss: 0.0112\n",
            "Epoch 24/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0105 - val_loss: 0.0110\n",
            "Epoch 25/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0120 - val_loss: 0.0108\n",
            "Epoch 26/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 27/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0108 - val_loss: 0.0105\n",
            "Epoch 28/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0095 - val_loss: 0.0104\n",
            "Epoch 29/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0110 - val_loss: 0.0102\n",
            "Epoch 30/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0108 - val_loss: 0.0101\n",
            "Epoch 31/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0114 - val_loss: 0.0100\n",
            "Epoch 32/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0099 - val_loss: 0.0098\n",
            "Epoch 33/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0098 - val_loss: 0.0097\n",
            "Epoch 34/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0097 - val_loss: 0.0096\n",
            "Epoch 35/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0092 - val_loss: 0.0094\n",
            "Epoch 36/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0097 - val_loss: 0.0093\n",
            "Epoch 37/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0088 - val_loss: 0.0092\n",
            "Epoch 38/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0084 - val_loss: 0.0091\n",
            "Epoch 39/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0090 - val_loss: 0.0090\n",
            "Epoch 40/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0087 - val_loss: 0.0088\n",
            "Epoch 41/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0093 - val_loss: 0.0087\n",
            "Epoch 42/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0082 - val_loss: 0.0086\n",
            "Epoch 43/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0086 - val_loss: 0.0085\n",
            "Epoch 44/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0080 - val_loss: 0.0084\n",
            "Epoch 45/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0074 - val_loss: 0.0083\n",
            "Epoch 46/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0075 - val_loss: 0.0082\n",
            "Epoch 47/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0072 - val_loss: 0.0081\n",
            "Epoch 48/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0080 - val_loss: 0.0080\n",
            "Epoch 49/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0079 - val_loss: 0.0079\n",
            "Epoch 50/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0074 - val_loss: 0.0079\n",
            "Epoch 51/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0072 - val_loss: 0.0078\n",
            "Epoch 52/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0070 - val_loss: 0.0077\n",
            "Epoch 53/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0077 - val_loss: 0.0076\n",
            "Epoch 54/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0075 - val_loss: 0.0075\n",
            "Epoch 55/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0065 - val_loss: 0.0075\n",
            "Epoch 56/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0072 - val_loss: 0.0074\n",
            "Epoch 57/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0062 - val_loss: 0.0073\n",
            "Epoch 58/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0074 - val_loss: 0.0073\n",
            "Epoch 59/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0070 - val_loss: 0.0072\n",
            "Epoch 60/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0070 - val_loss: 0.0071\n",
            "Epoch 61/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0074 - val_loss: 0.0071\n",
            "Epoch 62/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0070 - val_loss: 0.0070\n",
            "Epoch 63/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0073 - val_loss: 0.0069\n",
            "Epoch 64/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - val_loss: 0.0069\n",
            "Epoch 65/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0063 - val_loss: 0.0068\n",
            "Epoch 66/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0068 - val_loss: 0.0068\n",
            "Epoch 67/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0062 - val_loss: 0.0067\n",
            "Epoch 68/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058 - val_loss: 0.0067\n",
            "Epoch 69/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0063 - val_loss: 0.0066\n",
            "Epoch 70/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0066 - val_loss: 0.0066\n",
            "Epoch 71/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0062 - val_loss: 0.0065\n",
            "Epoch 72/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0067 - val_loss: 0.0065\n",
            "Epoch 73/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0064 - val_loss: 0.0064\n",
            "Epoch 74/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0059 - val_loss: 0.0064\n",
            "Epoch 75/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0060 - val_loss: 0.0064\n",
            "Epoch 76/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0060 - val_loss: 0.0063\n",
            "Epoch 77/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060 - val_loss: 0.0063\n",
            "Epoch 78/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0063 - val_loss: 0.0062\n",
            "Epoch 79/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - val_loss: 0.0062\n",
            "Epoch 80/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0057 - val_loss: 0.0062\n",
            "Epoch 81/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - val_loss: 0.0062\n",
            "Epoch 82/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0066 - val_loss: 0.0061\n",
            "Epoch 83/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - val_loss: 0.0061\n",
            "Epoch 84/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0061\n",
            "Epoch 85/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0062 - val_loss: 0.0060\n",
            "Epoch 86/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0060\n",
            "Epoch 87/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0063 - val_loss: 0.0060\n",
            "Epoch 88/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0060\n",
            "Epoch 89/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0059\n",
            "Epoch 90/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0060 - val_loss: 0.0059\n",
            "Epoch 91/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0059\n",
            "Epoch 92/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0061 - val_loss: 0.0059\n",
            "Epoch 93/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - val_loss: 0.0059\n",
            "Epoch 94/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - val_loss: 0.0058\n",
            "Epoch 95/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0058\n",
            "Epoch 96/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0067 - val_loss: 0.0058\n",
            "Epoch 97/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - val_loss: 0.0058\n",
            "Epoch 98/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - val_loss: 0.0058\n",
            "Epoch 99/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0059 - val_loss: 0.0058\n",
            "Epoch 100/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - val_loss: 0.0057\n",
            "Epoch 101/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0057\n",
            "Epoch 102/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - val_loss: 0.0057\n",
            "Epoch 103/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0057\n",
            "Epoch 104/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0059 - val_loss: 0.0057\n",
            "Epoch 105/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0057\n",
            "Epoch 106/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0058 - val_loss: 0.0057\n",
            "Epoch 107/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0059 - val_loss: 0.0056\n",
            "Epoch 108/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0056\n",
            "Epoch 109/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - val_loss: 0.0056\n",
            "Epoch 110/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - val_loss: 0.0056\n",
            "Epoch 111/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0057 - val_loss: 0.0056\n",
            "Epoch 112/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0056\n",
            "Epoch 113/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - val_loss: 0.0056\n",
            "Epoch 114/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0056 - val_loss: 0.0056\n",
            "Epoch 115/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - val_loss: 0.0055\n",
            "Epoch 116/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0056 - val_loss: 0.0055\n",
            "Epoch 117/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - val_loss: 0.0055\n",
            "Epoch 118/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - val_loss: 0.0055\n",
            "Epoch 119/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0053 - val_loss: 0.0055\n",
            "Epoch 120/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0055\n",
            "Epoch 121/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0059 - val_loss: 0.0055\n",
            "Epoch 122/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - val_loss: 0.0055\n",
            "Epoch 123/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0055\n",
            "Epoch 124/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - val_loss: 0.0055\n",
            "Epoch 125/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 126/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0054\n",
            "Epoch 127/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0046 - val_loss: 0.0054\n",
            "Epoch 128/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0059 - val_loss: 0.0054\n",
            "Epoch 129/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 130/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 131/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 132/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0058 - val_loss: 0.0054\n",
            "Epoch 133/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0056 - val_loss: 0.0054\n",
            "Epoch 134/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0059 - val_loss: 0.0054\n",
            "Epoch 135/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 136/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - val_loss: 0.0054\n",
            "Epoch 137/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - val_loss: 0.0054\n",
            "Epoch 138/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0053\n",
            "Epoch 139/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0060 - val_loss: 0.0053\n",
            "Epoch 140/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - val_loss: 0.0053\n",
            "Epoch 141/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0056 - val_loss: 0.0053\n",
            "Epoch 142/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0053\n",
            "Epoch 143/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 144/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 145/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0053\n",
            "Epoch 146/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0053\n",
            "Epoch 147/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0051 - val_loss: 0.0053\n",
            "Epoch 148/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 149/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 150/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - val_loss: 0.0053\n",
            "Epoch 151/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0051 - val_loss: 0.0053\n",
            "Epoch 152/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0053\n",
            "Epoch 153/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0052\n",
            "Epoch 154/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0059 - val_loss: 0.0052\n",
            "Epoch 155/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - val_loss: 0.0052\n",
            "Epoch 156/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - val_loss: 0.0052\n",
            "Epoch 157/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0052\n",
            "Epoch 158/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0052\n",
            "Epoch 159/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0052\n",
            "Epoch 160/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0052\n",
            "Epoch 161/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0052 - val_loss: 0.0052\n",
            "Epoch 162/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - val_loss: 0.0052\n",
            "Epoch 163/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0052\n",
            "Epoch 164/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0052\n",
            "Epoch 165/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0052\n",
            "Epoch 166/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0052\n",
            "Epoch 167/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0052\n",
            "Epoch 168/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0052\n",
            "Epoch 169/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - val_loss: 0.0051\n",
            "Epoch 170/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 171/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 172/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0051\n",
            "Epoch 173/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0051\n",
            "Epoch 174/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 175/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0051\n",
            "Epoch 176/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060 - val_loss: 0.0051\n",
            "Epoch 177/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 178/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0051\n",
            "Epoch 179/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 180/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 181/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 182/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 183/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0050\n",
            "Epoch 184/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0051 - val_loss: 0.0050\n",
            "Epoch 185/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0050\n",
            "Epoch 186/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 187/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0051 - val_loss: 0.0050\n",
            "Epoch 188/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0050\n",
            "Epoch 189/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 190/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 191/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 192/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 193/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0050\n",
            "Epoch 194/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - val_loss: 0.0050\n",
            "Epoch 195/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0050\n",
            "Epoch 196/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - val_loss: 0.0050\n",
            "Epoch 197/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 198/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0050\n",
            "Epoch 199/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 200/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0050\n",
            "Epoch 201/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0050\n",
            "Epoch 202/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 203/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 204/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0049\n",
            "Epoch 205/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0049\n",
            "Epoch 206/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0049\n",
            "Epoch 207/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - val_loss: 0.0049\n",
            "Epoch 208/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0049\n",
            "Epoch 209/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0043 - val_loss: 0.0049\n",
            "Epoch 210/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 211/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0049\n",
            "Epoch 212/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0049\n",
            "Epoch 213/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0049\n",
            "Epoch 214/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0052 - val_loss: 0.0049\n",
            "Epoch 215/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0049\n",
            "Epoch 216/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - val_loss: 0.0049\n",
            "Epoch 217/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - val_loss: 0.0048\n",
            "Epoch 218/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0048\n",
            "Epoch 219/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0048\n",
            "Epoch 220/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 221/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0048\n",
            "Epoch 222/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0048\n",
            "Epoch 223/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0045 - val_loss: 0.0048\n",
            "Epoch 224/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0048\n",
            "Epoch 225/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0048\n",
            "Epoch 226/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0048\n",
            "Epoch 227/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0048\n",
            "Epoch 228/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0048\n",
            "Epoch 229/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0048\n",
            "Epoch 230/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0048\n",
            "Epoch 231/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0048\n",
            "Epoch 232/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0048\n",
            "Epoch 233/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0048\n",
            "Epoch 234/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - val_loss: 0.0047\n",
            "Epoch 235/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 236/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0047\n",
            "Epoch 237/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - val_loss: 0.0047\n",
            "Epoch 238/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 239/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0047\n",
            "Epoch 240/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0047\n",
            "Epoch 241/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0047\n",
            "Epoch 242/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0047\n",
            "Epoch 243/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - val_loss: 0.0047\n",
            "Epoch 244/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0047\n",
            "Epoch 245/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 246/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 247/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0047\n",
            "Epoch 248/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0042 - val_loss: 0.0047\n",
            "Epoch 249/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 250/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0047\n",
            "Epoch 251/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0047\n",
            "Epoch 252/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - val_loss: 0.0046\n",
            "Epoch 253/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 254/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - val_loss: 0.0046\n",
            "Epoch 255/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 256/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - val_loss: 0.0046\n",
            "Epoch 257/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 258/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0048 - val_loss: 0.0046\n",
            "Epoch 259/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 260/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0046\n",
            "Epoch 261/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0046\n",
            "Epoch 262/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 263/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0046\n",
            "Epoch 264/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 265/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0046\n",
            "Epoch 266/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 267/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0041 - val_loss: 0.0046\n",
            "Epoch 268/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0045\n",
            "Epoch 269/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0046\n",
            "Epoch 270/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 271/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0045\n",
            "Epoch 272/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0045\n",
            "Epoch 273/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 274/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0045\n",
            "Epoch 275/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 276/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0045\n",
            "Epoch 277/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 278/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 279/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - val_loss: 0.0045\n",
            "Epoch 280/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0045\n",
            "Epoch 281/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0045\n",
            "Epoch 282/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0045 - val_loss: 0.0045\n",
            "Epoch 283/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0039 - val_loss: 0.0044\n",
            "Epoch 284/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 285/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 286/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 287/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 288/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0041 - val_loss: 0.0044\n",
            "Epoch 289/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0044\n",
            "Epoch 290/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0038 - val_loss: 0.0044\n",
            "Epoch 291/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 292/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0044\n",
            "Epoch 293/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0044\n",
            "Epoch 294/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - val_loss: 0.0044\n",
            "Epoch 295/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0044\n",
            "Epoch 296/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0040 - val_loss: 0.0043\n",
            "Epoch 297/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 298/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - val_loss: 0.0043\n",
            "Epoch 299/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 300/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 301/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 302/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0043\n",
            "Epoch 303/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 304/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - val_loss: 0.0043\n",
            "Epoch 305/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 306/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 307/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 308/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0043\n",
            "Epoch 309/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 310/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - val_loss: 0.0043\n",
            "Epoch 311/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - val_loss: 0.0042\n",
            "Epoch 312/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - val_loss: 0.0042\n",
            "Epoch 313/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 314/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0042\n",
            "Epoch 315/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 316/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0042\n",
            "Epoch 317/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 318/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 319/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - val_loss: 0.0042\n",
            "Epoch 320/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 321/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0042\n",
            "Epoch 322/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0042\n",
            "Epoch 323/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0042\n",
            "Epoch 324/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 325/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0042\n",
            "Epoch 326/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - val_loss: 0.0041\n",
            "Epoch 327/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 328/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0041\n",
            "Epoch 329/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0048 - val_loss: 0.0041\n",
            "Epoch 330/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 331/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0042\n",
            "Epoch 332/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0041\n",
            "Epoch 333/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0041\n",
            "Epoch 334/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 335/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0041\n",
            "Epoch 336/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0041\n",
            "Epoch 337/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0041\n",
            "Epoch 338/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0041\n",
            "Epoch 339/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0045 - val_loss: 0.0041\n",
            "Epoch 340/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 341/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0041\n",
            "Epoch 342/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 343/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0041\n",
            "Epoch 344/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0041\n",
            "Epoch 345/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0040\n",
            "Epoch 346/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 347/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 348/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 349/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - val_loss: 0.0040\n",
            "Epoch 350/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 351/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 352/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0038 - val_loss: 0.0040\n",
            "Epoch 353/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 354/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 355/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - val_loss: 0.0040\n",
            "Epoch 356/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 357/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0040\n",
            "Epoch 358/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0040\n",
            "Epoch 359/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0040\n",
            "Epoch 360/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 361/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0040\n",
            "Epoch 362/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0035 - val_loss: 0.0040\n",
            "Epoch 363/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0040\n",
            "Epoch 364/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0040\n",
            "Epoch 365/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - val_loss: 0.0040\n",
            "Epoch 366/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - val_loss: 0.0040\n",
            "Epoch 367/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0039\n",
            "Epoch 368/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 369/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 370/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 371/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 372/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0039\n",
            "Epoch 373/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0039\n",
            "Epoch 374/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0039\n",
            "Epoch 375/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - val_loss: 0.0039\n",
            "Epoch 376/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0039\n",
            "Epoch 377/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - val_loss: 0.0039\n",
            "Epoch 378/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 379/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0040\n",
            "Epoch 380/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 381/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0039\n",
            "Epoch 382/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 383/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 384/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 385/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 386/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0039\n",
            "Epoch 387/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - val_loss: 0.0039\n",
            "Epoch 388/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 389/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036 - val_loss: 0.0039\n",
            "Epoch 390/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 391/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - val_loss: 0.0039\n",
            "Epoch 392/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0046 - val_loss: 0.0039\n",
            "Epoch 393/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 394/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 395/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0039\n",
            "Epoch 396/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 397/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - val_loss: 0.0039\n",
            "Epoch 398/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - val_loss: 0.0039\n",
            "Epoch 399/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 400/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0038\n",
            "Epoch 401/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 402/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 403/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 404/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 405/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0039\n",
            "Epoch 406/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0038\n",
            "Epoch 407/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 408/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 409/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 410/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 411/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 412/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 413/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 414/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 415/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 416/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 417/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 418/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0038\n",
            "Epoch 419/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0043 - val_loss: 0.0038\n",
            "Epoch 420/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 421/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 422/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0038\n",
            "Epoch 423/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 424/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0038\n",
            "Epoch 425/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0038\n",
            "Epoch 426/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 427/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 428/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0038\n",
            "Epoch 429/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 430/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 431/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 432/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 433/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 434/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 435/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 436/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 437/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 438/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 439/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 440/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 441/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 442/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0038\n",
            "Epoch 443/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 444/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0038\n",
            "Epoch 445/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 446/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0038\n",
            "Epoch 447/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0042 - val_loss: 0.0038\n",
            "Epoch 448/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 449/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 450/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 451/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - val_loss: 0.0037\n",
            "Epoch 452/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0038\n",
            "Epoch 453/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 454/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 455/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 456/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 457/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 458/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 459/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 460/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 461/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 462/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 463/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 464/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 465/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0028 - val_loss: 0.0037\n",
            "Epoch 466/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 467/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 468/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 469/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 470/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 471/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 472/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 473/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 - val_loss: 0.0037\n",
            "Epoch 474/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - val_loss: 0.0037\n",
            "Epoch 475/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0032 - val_loss: 0.0037\n",
            "Epoch 476/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 477/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 478/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - val_loss: 0.0037\n",
            "Epoch 479/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0037\n",
            "Epoch 480/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - val_loss: 0.0037\n",
            "Epoch 481/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - val_loss: 0.0037\n",
            "Epoch 482/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 483/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 484/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0041 - val_loss: 0.0037\n",
            "Epoch 485/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0037\n",
            "Epoch 486/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - val_loss: 0.0037\n",
            "Epoch 487/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 488/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 489/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 490/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 491/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0038 - val_loss: 0.0037\n",
            "Epoch 492/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0037\n",
            "Epoch 493/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 494/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 495/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 496/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - val_loss: 0.0037\n",
            "Epoch 497/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - val_loss: 0.0037\n",
            "Epoch 498/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 499/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0030 - val_loss: 0.0037\n",
            "Epoch 500/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - val_loss: 0.0037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSXYsw65uLnU",
        "outputId": "c159ad78-2135-4b9a-8966-811087a77fbe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtsScF_6uSqF",
        "outputId": "d535efac-26a5-405f-be91-57f7f52d4d08"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8258154540015259"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "J2kRLIDVuwPn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "0HfcUnTXvUA2",
        "outputId": "eaa3f3ea-c3bc-410e-da2f-fd8ed421d33c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X86xCw8mvaqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}